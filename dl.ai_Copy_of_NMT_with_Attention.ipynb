{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of NMT with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kintumiku/hse_nlp/blob/master/dl.ai_Copy_of_NMT_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6CkVBp3XpXb",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1:  Neural Machine Translation\n",
        "\n",
        "Welcome to the second assignment of course 4. In this assignment you will explore neural machine translation using LSTMs with attention. You will implement an encoder-decoder algorithm from scratch and you will be able to to translate english sentences into German. \n",
        "\n",
        "Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation. For example, if you are not sure whether the word bank in a sentence refers to the financial bank, or the land alongside a river, then you can translate the sentence to see if it was translated to `banc` or `banque`. Regardless of this application, neural machine translation is extremely useful. By completing this assignment you will learn to:  \n",
        "\n",
        "- Learn how to use built in functions to preprocess your data\n",
        "- Implement an encoder-decoder system\n",
        "- Understand how attention works\n",
        "- Build the model from scratch\n",
        "- Evaluate your model\n",
        "- Translate using your own sentence\n",
        "\n",
        "As you can tell, this model is slightly different than the ones you have already implemented. This one uses an LSTM to encode a sentence and represent it into a context vector. It then inputs that context vector as a hidden layer into another LSTM and finally uses the LSTM to decode it into the target translation by keeping track of the attention. Don't worry about, we will explain everything in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrwKG8RxXpXd",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Importing the Data\n",
        "\n",
        "We will first start by importing a few packages and the data to do some exploration. We have provided for you a bunch of built in functions that you can use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiL_rGhFvFl",
        "colab_type": "text"
      },
      "source": [
        "If the next line throws an error, run it again (it should work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXcUxR6WXxHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d60974c5-3a23-456f-83fd-e3124c136192"
      },
      "source": [
        "!pip install -q -U trax==1.2.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 430kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 19.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 20.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 47.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 60.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 307kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 58.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 47.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.2 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6HtRbNNXpXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import gin\n",
        "from tensor2tensor import problems\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from termcolor import colored\n",
        "import tensorflow as tf\n",
        "#from utils import PrepareAttentionInput\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTY9JrhUqCFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = os.path.expanduser(\"data\")\n",
        "!rm -rf {data_dir}\n",
        "!mkdir {data_dir}\n",
        "tmp_dir = os.path.expanduser(\"tmp\")\n",
        "!rm -rf {tmp_dir}\n",
        "!mkdir {tmp_dir}\n",
        "\n",
        "# Fetch the en-de translation data.\n",
        "# The generate_data method of a problem will download data and process it into\n",
        "# a standard format ready for training and evaluation.\n",
        "ende_problem = problems.problem(\"translate_ende_wmt32k\")\n",
        "# ende_problem.generate_data(data_dir, tmp_dir)\n",
        "\n",
        "# Instead of the above commented-out line, for speed, we copy the data. \n",
        "!gsutil -q cp gs://tensor2tensor-data/tmp/vocab.translate_ende* {data_dir}\n",
        "!gsutil -m -q cp gs://tensor2tensor-data/tmp/translate_ende* {data_dir}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tPhIxD7cQgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoders = ende_problem.feature_encoders('./data/')  # Tokenizer: base of subword modeling."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ka38XF5XpXh",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Encode & Decode helper functions\n",
        "\n",
        "\n",
        "The cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
        "\n",
        "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
        "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
        "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
        "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
        "\n",
        "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
        "\n",
        "- <span style='color:blue'> encode: </span> converts a text sentence to its corresponding token array (i.e. list of indices). Also converts words to subwords.\n",
        "- <span style='color:blue'> decode: </span> converts a token array to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Fj8bzZXpXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode function\n",
        "def encode(input_str, output_str=None):\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "    EOS = 1\n",
        "    inputs =  encoders[\"inputs\"].encode(input_str)+ [EOS]  # Add EOS index to mark the end of the sentence (input)\n",
        "    batch_inputs = np.reshape(inputs, [1, -1])  # Add the batch dimension to the front of the shape\n",
        "    return batch_inputs\n",
        "\n",
        "# decode function\n",
        "def decode(integers):\n",
        "    \"\"\"List of ints to str\"\"\"\n",
        "    integers = list(np.squeeze(integers)) # Remove the dimensions of size 1\n",
        "    EOS = 1\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)] # Remove the EOS index to decode only those tokens which have a translation\n",
        "    return encoders[\"inputs\"].decode(integers)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiYmMNOUXpXk",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Defining parameters\n",
        "\n",
        "You will now set a few hyperparameters that will allow you to  like batch_size, maximum batch length, and others that will allow you to train your model. The data preprocessing techniques we used are from a package known as Tensor2tensor (T2T) developed by the Google team. Run the cell below to set the hyperparameters using the T2T package. This method uses `gin`, which is a lightweight configuration framework that allows you to set the default parameters. Take a look to see how to set the parameters [here](https://github.com/google/gin-config/blob/master/docs/index.md). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCK9pld2XpXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters how to batch T2T problem.\n",
        "gin.bind_parameter('batch_fn.batch_size_per_device', 64)\n",
        "gin.bind_parameter('batch_fn.eval_batch_size', 64)\n",
        "gin.bind_parameter('batch_fn.max_eval_length', 512)\n",
        "gin.bind_parameter('batch_fn.bucket_length', 32)\n",
        "gin.bind_parameter('batch_fn.buckets_include_inputs_in_length', True)\n",
        "# Preprocessing just cuts off too long sequences.\n",
        "gin.bind_parameter('shuffle_and_batch_data.preprocess_fun',\n",
        "                   trax.supervised.inputs.wmt_preprocess)\n",
        "gin.bind_parameter('wmt_preprocess.max_length', 256)\n",
        "gin.bind_parameter('wmt_preprocess.max_eval_length', 512)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usbry-DbXpXn",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Exploring the data\n",
        "\n",
        "Now that you have defined your parameters above, you will see that the functions we gave you do the exact same thing you have been doing again and again throughout the specialization. We gave them to you so you can focus more on building the model from scratch. Feel free to see the tokenized versions of your data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC-AXGagXpXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shutting down the warnings. \n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore these 2 lines\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# These are the english to german inputs\n",
        "ende_inputs = trax.supervised.inputs.inputs(\"t2t_translate_ende_wmt32k\",\n",
        "                                            data_dir='./data/')\n",
        "# using your data generator to see the data \n",
        "input_batch, target_batch = next(ende_inputs.train_stream(1))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcQIM0neXpXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "455eccaa-4986-4d97-c3d7-dfef205d5b2b"
      },
      "source": [
        "# Test what it produces.\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), decode(input_batch[0]))\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ','red') , input_batch[0])\n",
        "print(colored('THIS IS THE GERMAN TRANSLATION: \\n','red'), decode(target_batch[0]))\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'),encode(decode(input_batch[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
            "\u001b[0m I would have liked to have had him here, as I would like to remind him of paragraph 6 of the resolution, in which we ask the services to go through the budget with a fine toothcomb with the aim of identifying potential savings.\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [   46    98    43  6855     9    43   214   802   439     2    36    46\n",
            "    98   151     9  6058   802     7  5766   330     7     4   998     2\n",
            "     6    65    60  1150     4   508     9   491   317     4   700    30\n",
            "    13  3835 22519 17702     5    30     4  2775     7 28866  6899  1424\n",
            "  5473     3     1     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
            "\u001b[0m Ich hätte ihn gerne hier mit dabei gehabt, und ich hätte ihn gerne an Absatz 6 der Entschließung erinnert, in dem wir die Dienststellen auffordern, den Haushaltsplan genau unter die Lupe zu nehmen, um potenzielle Einsparungen zu ermitteln.\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
            "\u001b[0m [[   46    98    43  6855     9    43   214   802   439     2    36    46\n",
            "     98   151     9  6058   802     7  5766   330     7     4   998     2\n",
            "      6    65    60  1150     4   508     9   491   317     4   700    30\n",
            "     13  3835 22519 17702     5    30     4  2775     7 28866  6899  1424\n",
            "   5473     3     1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxRyz5LnXpXt",
        "colab_type": "text"
      },
      "source": [
        "You can see that the `ende_inputs`, (the english to german data generator), returns batches of data. You can use `next` to get the next batch. Take a minute or two to understand the tokens. What does `1` correspond to? Why are we using the `0`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOHhFthvXpXu",
        "colab_type": "text"
      },
      "source": [
        "# 2.0 Neural Machine Translation with Attention\n",
        "\n",
        "Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you time because we know you already processed data before in this specialization, so we would rather you spend your time doing the cool stuff. You will be implementing a neural machine translation model from scratch with attention. Concretely, you will understand how the encoder works, how the decoder works, and how the attentions heads are used. \n",
        "\n",
        "### 2.1 Encoder \n",
        "\n",
        "- <span style=\"color:red\">input_encoder </span>: Input encoder runs on the English sentence and creates activations that will be the keys and values for attention. This is a serial function which uses:\n",
        "<ul>\n",
        "    <li> \n",
        "        <span style=\"color:blue\">t1.Embedding </span>: takes in the dimension of the model and the input vocab size\n",
        "   </li> \n",
        "   <li> \n",
        "        An array of n_encoder_layers of LSTMs each of dimension d_model\n",
        "   </li> \n",
        "</ul>\n",
        "\n",
        "- <span style=\"color:red\">pre-attention input </span>: Prepares the queries keys values\n",
        "<ul>\n",
        "    <li> \n",
        "        <span style=\"color:blue\">t1.Embedding </span>: \n",
        "   </li> \n",
        "   <li> \n",
        "        An array of n_encoder_layers of LSTMs each of dimension d_model\n",
        "   </li> \n",
        "</ul>\n",
        "\n",
        "### 2.2 Attention Explained\n",
        "\n",
        "<img src = \"att.png\">\n",
        "\n",
        "Over here we will denote the queries, Q, the keys K, and the values V. We learn these embeddings by learning $W_Q, W_K, W_V$. In the image above, imagine that K is they key, say the english word embedding and Q is the query say, the german word embedding. The dot product of similar vectors tend to have higher values. So think of it as a similarity operation. If the query and the key are similar, then the dot product will be similar. If it is different, then it will be small. Then you take the softmax of that, and multiply it by V. You can think of V as the value or the english word embedding. And then that tells you how much weight to put on each english word when translating. So each word, has its key.\n",
        "\n",
        "A query q will assign each key a probability that that key k is a match for q. We measure similarity by taking dot product of vectors: so q and k are similar of qk is large. These similarity numbers do not add up to 1, so they cannot be used as probabilities. To make them so, and to make attention more focused on the best matching keys, we use the softmax -- the same as in the cross-entropy loss. So you can compute the matrix of query-key probabilities, often called the “attention weights”, just as softmax(QK^T). This matrix has shape L_Q by L_K -- each query and key pair gets a probability.\n",
        "\n",
        "In the final step, we take the values - which is another matrix of the same shape as keys, and often the same as keys, and we want to get a weighted sum, weighting each value v_i by the probability that the key k_i matches the query. This can be computed very efficiently just as matrix multiplication: we multipy attention weights by values, that’s it!\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vDK8d-QXpXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepareAttentionInput(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\"\"\"\n",
        "    keys = values = encoder_activations\n",
        "    queries = decoder_activations\n",
        "    \n",
        "    # Mask is 1 where inputs are not padding (0) and 0 where they are padding.\n",
        "    mask = (inputs != 0)\n",
        "    # We need to add axes to the mask for attention heads and decoder length.\n",
        "    mask = trax.math.numpy.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    # Broadcast so mask shape is [batch, 1 for heads, decoder-len, encoder-len].\n",
        "    mask = mask + trax.math.numpy.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "    return queries, keys, values, mask"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIAIQhzVXpXx",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Decoder \n",
        "\n",
        "<ul>\n",
        "    <li> \n",
        "        <span style=\"color:blue\">tl.LSTM:  </span> takes in the dimension of the model: \n",
        "   </li> \n",
        "   <li> \n",
        "        An array of n_decoder_layers of LSTMs each of dimension d_model\n",
        "   </li> \n",
        "    <li> \n",
        "        <span style=\"color:blue\">tl.Dense:  </span> the dense layer \n",
        "   </li> \n",
        "    <li> \n",
        "        <span style=\"color:blue\">tl.logsoftmax:  </span> of dimension V for the vocabulary\n",
        "   </li> \n",
        "</ul>\n",
        "\n",
        "\n",
        "\n",
        "By completing this part you complete the following functions: \n",
        "\n",
        "### 2.4 Implementation Overview\n",
        "\n",
        "\n",
        "- <span style=\"color:red\">Pre_attention_decoder </span>: runs on the targets and creates activations that are used as queries in attention. This is a serial function which takes the following:\n",
        "<ul>\n",
        "    <li> \n",
        "        <span style=\"color:blue\"> tl.ShiftRight(mode=mode) </span>: takes in the dimension of the model and the input vocab size\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Embedding </span>: takes in the dimension of the model and the vocab target size\n",
        "   </li> \n",
        "</ul>\n",
        "- <span style=\"color:red\">Model:</span> the final model\n",
        "<ul>\n",
        "    <li> \n",
        "        <span style=\"color:blue\"> tl.Select </span>: duplicate the input so you can keep track of it when doing the attention\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Parallel </span>: Run input encoder on the input and pre-attention decoder the target\n",
        "   </li> \n",
        "    \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Fn(PrepareAttentionInput, n_out=4) </span>: Prepare queries, keys, values and mask for attention.\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Residual(tl.AttentionQKV(...)) </span>: Run the attention layer, add to the pre-attention decoder\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Select(...) </span>: drop the attention mask\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> Array of LSTMs</span>: run the rest of the decoder\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.Dense </span>: Prepare output by making it the right size\n",
        "   </li> \n",
        "   <li> \n",
        "        <span style=\"color:blue\"> tl.LogSoftmax </span>: Log-softmax for output.\n",
        "   </li> \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKQRG688XpXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NMT model with attention.\n",
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "        n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "        n_attention_heads: int: number of attention heads\n",
        "        attention_dropout: float, dropout for the attention layer\n",
        "        mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "        A LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "    input_encoder = tl.Serial(\n",
        "      tl.Embedding(d_model, input_vocab_size),\n",
        "      [tl.LSTM(d_model) for _ in range(n_encoder_layers)],\n",
        "    )\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "      tl.ShiftRight(mode=mode),  # Teacher forcing: predict O_t from O_{t-1}...\n",
        "      tl.Embedding(d_model, target_vocab_size),\n",
        "      tl.LSTM(d_model)\n",
        "    )\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "    \n",
        "    model = tl.Serial(\n",
        "      # Copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "      # Run input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "      # Prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn(PrepareAttentionInput, n_out=4),\n",
        "      # Run the attention layer, add to the pre-attention decoder.\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads,\n",
        "                                  dropout=attention_dropout, mode=mode)),\n",
        "      tl.Select([0], n_in=2),  # Drop attention mask (not needed).\n",
        "      # Run the rest of the RNN decoder.\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
        "      # Prepare output by making it the right size.\n",
        "      tl.Dense(target_vocab_size),\n",
        "      # Log-softmax for output.\n",
        "      tl.LogSoftmax()\n",
        "  )\n",
        "    return model\n",
        "  ### END CODE HERE ###"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhA6yfb2XpX0",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Training\n",
        "\n",
        "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In our case, you will train your model on a cpu and we will load in a pre-trained model for you. You could then train it a little bit more and predict with your own words. Instead of making you wait for a couple of hours, we will give you a trained version and then you will train it more using a program you are about to write. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2FOwpsXpX0",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Defining the optimizer and the loss function\n",
        "\n",
        "In this part you will define the optimizer and the loss function. You need two optimizers. The first one is for the encoder and the second one is for the decoder.\n",
        "\n",
        "You will be using stochastic gradient descent to update the parameters and a negative loglikelood function. \n",
        "\n",
        "$$J = - \\frac{1}{m}\\sum_{i=1}^m \\log (\\hat y^{(i)}) $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKJNpHORXpX1",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Training the model\n",
        "\n",
        "\n",
        "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator.\n",
        "\n",
        "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do: \n",
        "- Create the trainer object by calling `trax.supervised.Trainer` and pass in the following: \n",
        "    - <span style='color:blue'> model </span> = NMTAttn\n",
        "    - <span style='color:blue'> loss_fn </span>=tl.CrossEntropyLoss\n",
        "    - <span style='color:blue'> optimizer </span> =trax.optimizers.Adam\n",
        "    - <span style='color:blue'> lr_schedule </span>=trax.lr.MultifactorSchedule\n",
        "    - <span style='color:blue'> inputs</span>= ende_inputs\n",
        "    - <span style='color:blue'> output_dir</span>=output_dir\n",
        "    \n",
        "You will be using a cross entropy loss, with Adam optimizer. Please read the [trax](link) documentation to get a full understanding. Make sure you use `trainer.train_epoch(train_steps, eval_steps)` to train it the right number of times. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgQ-IQVDXpX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: train_model\n",
        "def train_model(NMTAttn, n_epochs, train_steps, eval_steps, output_dir = \"~/\"):\n",
        "    '''\n",
        "    Input: \n",
        "        NER - the model you are building\n",
        "        n_epochs - number of times to go over all the data\n",
        "        train_steps - number of training steps\n",
        "        eval_steps - the evaluation steps\n",
        "        output_dir - folder to save your file\n",
        "    '''\n",
        "    output_dir = os.path.expanduser(output_dir) # trainer is an object\n",
        "    lr_schedule = lambda h: trax.lr.MultifactorSchedule(\n",
        "    h, factors='constant * linear_warmup * rsqrt_decay', constant=0.05, warmup_steps=1000)\n",
        "    ### START CODE HERE ###\n",
        "    trainer = trax.supervised.Trainer(\n",
        "        model=NMTAttn,\n",
        "        loss_fn=tl.CrossEntropyLoss(),\n",
        "        optimizer=trax.optimizers.Adam,  \n",
        "        lr_schedule=lr_schedule,\n",
        "        inputs= ende_inputs, \n",
        "        output_dir=output_dir)\n",
        "    for _ in range(n_epochs):\n",
        "        trainer.train_epoch(train_steps, eval_steps)\n",
        "    ### END CODE HERE ###\n",
        "    return trainer\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqeMlPK3G-pP",
        "colab_type": "text"
      },
      "source": [
        "Notice the model is being trained for 1 epoch only, so it will be very different from the fully trained model (which you'll use later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz89quhVXpX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "a77625c3-79c9-4757-ff61-0a0dca32d3c6"
      },
      "source": [
        "output_dir = os.path.expanduser('~/output_dir/')\n",
        "n_epochs  = 1\n",
        "train_steps = 10\n",
        "eval_steps = 10\n",
        "trainer = train_model(NMTAttn, n_epochs, train_steps, eval_steps, output_dir)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step     10: Ran 10 train steps in 219.06 secs\n",
            "Step     10: Evaluation\n",
            "Step     10: train                   accuracy |  0.00000000\n",
            "Step     10: train                       loss |  10.22048855\n",
            "Step     10: train         neg_log_perplexity | -10.22048855\n",
            "Step     10: train          sequence_accuracy |  0.00000000\n",
            "Step     10: train weights_per_batch_per_core |  2048.00000000\n",
            "Step     10: eval                    accuracy |  0.00000000\n",
            "Step     10: eval                        loss |  10.22193718\n",
            "Step     10: eval          neg_log_perplexity | -10.22193718\n",
            "Step     10: eval           sequence_accuracy |  0.00000000\n",
            "Step     10: eval  weights_per_batch_per_core |  2048.00000000\n",
            "Step     10: Finished evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3v0J58fXpX7",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output:** The loss should be around 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7a1JauyXpX7",
        "colab_type": "text"
      },
      "source": [
        " # Part 4:  Evaluation  \n",
        "\n",
        "### 4.1 Loading in a trained model\n",
        "\n",
        "In this part you will evaluate by loading in the exact same version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9poGJ1NXpX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8625297d-60cf-4de6-b8bf-7fbfa702d9f8"
      },
      "source": [
        "model = NMTAttn(mode='eval')\n",
        "# Copy pre-trained model\n",
        "!gsutil cp gs://tensor2tensor-data/tmp/nmt/model.pkl* {tmp_dir}\n",
        "model.init_from_file( \"./tmp/model.pkl\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tensor2tensor-data/tmp/nmt/model.pkl...\n",
            "Copying gs://tensor2tensor-data/tmp/nmt/model.pkl.gz...\n",
            "/ [2 files][  3.2 GiB/  3.2 GiB]   41.4 MiB/s                                   \n",
            "Operation completed over 2 objects/3.2 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcbxwup4XpX-",
        "colab_type": "text"
      },
      "source": [
        "# Part 5: Testing with your own input\n",
        "\n",
        "You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. \n",
        "\n",
        "**Instructions:** Implement the next symbol function that takes in the input_tokens, and the cur_output_tokens and return the the index of the next word. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOmEsg0rXpX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoding functions.\n",
        "def next_symbol(input_tokens, cur_output_tokens):\n",
        "    padded_length = 2**int(np.ceil(np.log2(len(cur_output_tokens) + 3)))\n",
        "    padded = cur_output_tokens + [0] * (padded_length - len(cur_output_tokens))\n",
        "    padded_with_batch = np.array(padded)[None, :]\n",
        "    output, _ = model((input_tokens, padded_with_batch), n_accelerators=1)\n",
        "    log_probs = output[0, len(cur_output_tokens), :]\n",
        "    return int(np.argmax(log_probs))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC9Kvh_VXpYB",
        "colab_type": "text"
      },
      "source": [
        "Now you will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence and returns the the decoded sentence. \n",
        "\n",
        "**Instructions**: Implement the greedy_decode algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inoYkZaLXpYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "92d2b5bb-1eba-4330-fa6b-738ee7b830b7"
      },
      "source": [
        "def greedy_decode(input_sentence):\n",
        "    input_tokens = encode(input_sentence)\n",
        "    cur_output_tokens = []\n",
        "    cur_output = 0\n",
        "    EOS = 1\n",
        "    while cur_output != EOS:\n",
        "        cur_output = next_symbol(input_tokens, cur_output_tokens)\n",
        "        cur_output_tokens.append(cur_output)\n",
        "    return decode(cur_output_tokens)\n",
        "\n",
        "sentence = \"I am hungry.\"\n",
        "print(sentence)\n",
        "print(greedy_decode(sentence))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am hungry.\n",
            "Ich bin hungrig.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}